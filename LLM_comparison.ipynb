{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1AxLkA_rsuk_rsFXsKHzHy51iocggR9fU",
      "authorship_tag": "ABX9TyM2K3mJlagZJT3bJy5bY47R"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# DATA"
      ],
      "metadata": {
        "id": "Ko-WdKWLT-Wy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "\"\"\"\n",
        "FILE FORMAT: csv\n",
        "Required columns: question, context\n",
        "Optional columns: history\n",
        "\"\"\"\n",
        "file_path = '<file_path>",
        "\n",
        "# Read the CSV file into a DataFrame\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iUndkhvNaEut",
        "outputId": "5937d5c4-3056-4a12-ddaf-d5ee7d3f8d7b"
      },
      "execution_count": null,
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PROMPTS"
      ],
      "metadata": {
        "id": "InU-iVlMUB3g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"<system prompt>\""
      ],
      "metadata": {
        "id": "EVzJYHv2P8Xj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CLIENTS"
      ],
      "metadata": {
        "id": "aB96bTcOUEzA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JLkL2Vn95Xme"
      },
      "outputs": [],
      "source": [
        "api_keys = {\n",
        "    \"gemini\": \"<key>\",\n",
        "    \"anthropic\": \"<key>\",\n",
        "    \"groq\": \"<key>\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install anthropic\n",
        "!pip install groq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UwMwocJJUdS8",
        "outputId": "e3f3a875-25d4-4799-d5b3-37091fbfcd04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: anthropic in /usr/local/lib/python3.11/dist-packages (0.49.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (0.9.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from anthropic) (2.10.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from anthropic) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from anthropic) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->anthropic) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->anthropic) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->anthropic) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->anthropic) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->anthropic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->anthropic) (2.27.2)\n",
            "Requirement already satisfied: groq in /usr/local/lib/python3.11/dist-packages (0.20.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from groq) (2.10.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from groq) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (2.27.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google import genai\n",
        "import anthropic\n",
        "from groq import Groq\n",
        "\n",
        "clients = {\n",
        "    \"groq\": Groq(api_key=api_keys.get(\"groq\")),\n",
        "    \"gemini\": genai.Client(api_key=api_keys.get(\"gemini\")),\n",
        "    \"anthropic\": anthropic.Anthropic(api_key=api_keys.get(\"anthropic\"))\n",
        "}\n"
      ],
      "metadata": {
        "id": "FXrC0lgobv4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "import traceback\n",
        "\n",
        "def post_process_llm_answer(string):\n",
        "    if string is None:\n",
        "        return {}\n",
        "    # remove all characters before the first {\n",
        "    string = re.sub(r\"^[^{]*{\", \"{\", string)\n",
        "    # remove all characters after the last }\n",
        "    string = re.sub(r\"}[^}]*$\", \"}\", string)\n",
        "    result = json.loads(string)\n",
        "    return result\n",
        "\n",
        "def analyse_func(func):\n",
        "    def wrapper(*args, **kwargs):\n",
        "        start_time = time.time()\n",
        "        result = func(*args, **kwargs)\n",
        "        end_time = time.time()\n",
        "        time_taken = end_time - start_time\n",
        "        analysis = {\n",
        "            \"provider\": func.__name__.split(\"_\")[0],\n",
        "            \"model\": kwargs.get(\"model\"),\n",
        "            \"time\": round(time_taken, 4)\n",
        "        }\n",
        "        analysis.update(result)\n",
        "        return analysis\n",
        "    return wrapper\n",
        "\n",
        "@analyse_func\n",
        "def gemini_call(system_prompt, user_prompt, context, history, query, model=\"gemini-2.0-flash\"):\n",
        "    user_prompt = user_prompt.format(context=context, history=history, query=query)\n",
        "    response = clients[\"gemini\"].models.generate_content(\n",
        "        model=model,\n",
        "        contents=user_prompt,\n",
        "        config=genai.types.GenerateContentConfig(\n",
        "            system_instruction=system_prompt,\n",
        "            temperature=0,\n",
        "            max_output_tokens=10000,\n",
        "            top_p=0,\n",
        "            top_k=1\n",
        "        )\n",
        "    )\n",
        "    result = post_process_llm_answer(response.text)\n",
        "    result[\"total_tokens\"] = response.usage_metadata.total_token_count\n",
        "    return result\n",
        "\n",
        "@analyse_func\n",
        "def anthropic_call(system_prompt, user_prompt, context, history, query, model=\"claude-3-7-sonnet-20250219\"):\n",
        "    user_prompt = user_prompt.format(context=context, history=history, query=query)\n",
        "    response = clients[\"anthropic\"].messages.create(\n",
        "        model=model,\n",
        "        max_tokens=10000,\n",
        "        temperature=0,\n",
        "        system=system_prompt,\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\n",
        "                        \"type\": \"text\",\n",
        "                        \"text\": user_prompt\n",
        "                    }\n",
        "                ]\n",
        "            }\n",
        "        ]\n",
        "    )\n",
        "    result = post_process_llm_answer(response.content[0].text)\n",
        "    return result\n",
        "\n",
        "@analyse_func\n",
        "def groq_call(system_prompt, user_prompt, context, history, query, model=\"qwen-2.5-32b\"):\n",
        "    user_prompt = user_prompt.format(context=context, history=history, query=query)\n",
        "    response = clients[\"groq\"].chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": system_prompt\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": user_prompt\n",
        "            }\n",
        "        ],\n",
        "        temperature=0,\n",
        "        max_completion_tokens=10000,\n",
        "        top_p=0,\n",
        "        stream=False,\n",
        "        response_format={\"type\": \"json_object\"},\n",
        "        stop=None,\n",
        "    )\n",
        "    result = post_process_llm_answer(response.choices[0].message.content)\n",
        "    result[\"total_tokens\"] = response.usage.total_tokens\n",
        "    result[\"queue_time\"] = round(response.usage.queue_time, 4)\n",
        "    result[\"gen_time\"] = round(response.usage.total_time, 4)\n",
        "    return result\n",
        "\n",
        "def test_llms(test_df, llms):\n",
        "    results = {}\n",
        "    for index, row in test_df.iterrows():\n",
        "        context = row.get(\"context\", \"\")\n",
        "        history = row.get(\"history\", \"\")\n",
        "        query = row.get(\"question\", \"\")\n",
        "        for provider, models in llms.items():\n",
        "            for model in models:\n",
        "                try:\n",
        "                    func = globals()[f\"{provider}_call\"]\n",
        "                    result = func(system_prompt, user_prompt, context, history, query, model=model)\n",
        "                    key = f\"{provider}_{model}\"\n",
        "                    if key not in results:\n",
        "                        results[key] = []\n",
        "                    result[\"question\"] = query\n",
        "                    result[\"context\"] = context\n",
        "                    result[\"history\"] = history\n",
        "                    results[key].append(result)\n",
        "                except Exception as e:\n",
        "                    print(f\"Exception occured for ({provider}, {model}): {e}\")\n",
        "                    print(traceback.format_exc())\n",
        "                    continue\n",
        "    return results\n",
        "\n",
        "def process_results(results, save=False, dir_path=\"/content/drive/MyDrive/Colab Notebooks/data/llm_results\"):\n",
        "    results_dfs = []\n",
        "    if save and not os.path.exists(dir_path):\n",
        "        os.makedirs(dir_path)\n",
        "    for model, result in results.items():\n",
        "        df = pd.DataFrame(result)\n",
        "        results_dfs.append(df)\n",
        "        if save:\n",
        "            df.to_csv(f\"{dir_path}/{model}.csv\", index=False)\n",
        "    return results_dfs"
      ],
      "metadata": {
        "id": "icIkcagC6KzU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# INFERENCE"
      ],
      "metadata": {
        "id": "rteou4tt4YKR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add the models to test to this dictionary and comment the ones you don't want to test\n",
        "llms_to_test = {\n",
        "    \"gemini\": [\n",
        "        # \"gemini-2.0-flash\",\n",
        "        \"gemini-2.0-flash-lite\"\n",
        "    ],\n",
        "    # \"anthropic\": [\"claude-3-7-sonnet-20250219\"],\n",
        "    # \"groq\": [\n",
        "    #     \"qwen-2.5-32b\",\n",
        "    #     \"llama-3.3-70b-versatile\",\n",
        "    # ]\n",
        "}"
      ],
      "metadata": {
        "id": "piXtD8p-6Rqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = test_llms(df, llms_to_test)\n"
      ],
      "metadata": {
        "id": "unqceicPJInh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dir_path=\"/content/drive/MyDrive/Colab Notebooks/data/llm_analysis/expt1\"\n",
        "results_dfs = process_results(results=results, save=True, dir_path=dir_path)"
      ],
      "metadata": {
        "id": "jwmLgktr9QiJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_dfs[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 644
        },
        "id": "H0oz5Z__ih_N",
        "outputId": "16785611-3e40-4c80-d5e2-e8f2b152d132"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "metadata": {
        "id": "Blu-fMyaonzL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h39MHMhAnGTC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
